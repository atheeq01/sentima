{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8740a8d732168e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.047058Z",
     "start_time": "2025-05-24T16:04:48.082717Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('../artifacts/allReviews.csv')\n",
    "df = df[df[\"Comment\"].notnull() & df[\"Rating\"].notnull()]\n",
    "df = df[df[\"Comment\"].notnull() | df[\"Rating\"].notnull()]\n",
    "df = df[[\"Comment\", \"Rating\"]]\n",
    "df_1 = df[df[\"Rating\"]==1]\n",
    "df_2 = df[df[\"Rating\"]==2]\n",
    "df_3 = df[df[\"Rating\"]==3]\n",
    "df_4 = df[df[\"Rating\"]==4].sample(n=16000,random_state=42)\n",
    "df_5 = df[df[\"Rating\"]==5].sample(n=16000,random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_1,df_2,df_3,df_4,df_5])\n",
    "df = df_balanced.sample(frac=1,random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bb7ef1a926f9768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.173936Z",
     "start_time": "2025-05-24T16:04:50.166058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95948</th>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95949</th>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95950</th>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95951</th>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95952</th>\n",
       "      <td>no me gusta el curso</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Comment  Rating\n",
       "0                                                   good     4.0\n",
       "1      Interesting introduction to the subject matter...     3.0\n",
       "2      I have learned the basic concepts of different...     5.0\n",
       "3      the course is really good but there are issues...     3.0\n",
       "4      Not much as I was expected, not very structure...     2.0\n",
       "...                                                  ...     ...\n",
       "95948  Overly-remedial information, but that is not w...     1.0\n",
       "95949  Content is quite interesting. Timing of the le...     3.0\n",
       "95950  This subject should be taught as a course in t...     4.0\n",
       "95951  I cannot get my certificate as ID could not be...     1.0\n",
       "95952                               no me gusta el curso     1.0\n",
       "\n",
       "[95953 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbf44f87576961e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.333544Z",
     "start_time": "2025-05-24T16:04:50.327665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating\n",
      "3.0    33718\n",
      "1.0    16337\n",
      "4.0    16000\n",
      "5.0    16000\n",
      "2.0    13898\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Rating\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "beeccc6584ceac7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.603802Z",
     "start_time": "2025-05-24T16:04:50.598742Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Sentiment\"]=df[\"Rating\"].map({\n",
    "    1:0,\n",
    "    2:0,\n",
    "    3:1,\n",
    "    4:2,\n",
    "    5:2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "515f9f9e8d8de4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.836251Z",
     "start_time": "2025-05-24T16:04:50.830243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95948</th>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95949</th>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95950</th>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95951</th>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95952</th>\n",
       "      <td>no me gusta el curso</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95953 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Comment  Rating  Sentiment\n",
       "0                                                   good     4.0          2\n",
       "1      Interesting introduction to the subject matter...     3.0          2\n",
       "2      I have learned the basic concepts of different...     5.0          2\n",
       "3      the course is really good but there are issues...     3.0          2\n",
       "4      Not much as I was expected, not very structure...     2.0          0\n",
       "...                                                  ...     ...        ...\n",
       "95948  Overly-remedial information, but that is not w...     1.0          0\n",
       "95949  Content is quite interesting. Timing of the le...     3.0          2\n",
       "95950  This subject should be taught as a course in t...     4.0          2\n",
       "95951  I cannot get my certificate as ID could not be...     1.0          0\n",
       "95952                               no me gusta el curso     1.0          0\n",
       "\n",
       "[95953 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9babf9432e71684",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:50.924757Z",
     "start_time": "2025-05-24T16:04:50.920578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "2    65718\n",
      "0    30235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8197d4435b0dbb",
   "metadata": {},
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5429e4c6a252ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:51.055741Z",
     "start_time": "2025-05-24T16:04:51.053155Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "\n",
    "def remove_urls_batch(text_series):\n",
    "    return text_series.str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "135bf76bb77569a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:51.769272Z",
     "start_time": "2025-05-24T16:04:51.218395Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_Comment'] = df['Comment'].apply(lambda x: re.sub(url_pattern, '', x, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1971220f1b072d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:51.785029Z",
     "start_time": "2025-05-24T16:04:51.783396Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = df.drop(columns=[\"contains_url\"])\n",
    "# df = df.drop(columns=[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11010f1c77ad8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:51.841590Z",
     "start_time": "2025-05-24T16:04:51.836395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95948</th>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95949</th>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95950</th>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95951</th>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95952</th>\n",
       "      <td>no me gusta el curso</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>no me gusta el curso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95953 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Comment  Rating  Sentiment  \\\n",
       "0                                                   good     4.0          2   \n",
       "1      Interesting introduction to the subject matter...     3.0          2   \n",
       "2      I have learned the basic concepts of different...     5.0          2   \n",
       "3      the course is really good but there are issues...     3.0          2   \n",
       "4      Not much as I was expected, not very structure...     2.0          0   \n",
       "...                                                  ...     ...        ...   \n",
       "95948  Overly-remedial information, but that is not w...     1.0          0   \n",
       "95949  Content is quite interesting. Timing of the le...     3.0          2   \n",
       "95950  This subject should be taught as a course in t...     4.0          2   \n",
       "95951  I cannot get my certificate as ID could not be...     1.0          0   \n",
       "95952                               no me gusta el curso     1.0          0   \n",
       "\n",
       "                                           clean_Comment  \n",
       "0                                                   good  \n",
       "1      Interesting introduction to the subject matter...  \n",
       "2      I have learned the basic concepts of different...  \n",
       "3      the course is really good but there are issues...  \n",
       "4      Not much as I was expected, not very structure...  \n",
       "...                                                  ...  \n",
       "95948  Overly-remedial information, but that is not w...  \n",
       "95949  Content is quite interesting. Timing of the le...  \n",
       "95950  This subject should be taught as a course in t...  \n",
       "95951  I cannot get my certificate as ID could not be...  \n",
       "95952                               no me gusta el curso  \n",
       "\n",
       "[95953 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cad67544f26b5a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:54.330427Z",
     "start_time": "2025-05-24T16:04:51.919086Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45c64223f768b9c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:56.096485Z",
     "start_time": "2025-05-24T16:04:54.366425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a32247b0f3ec0204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:56.377858Z",
     "start_time": "2025-05-24T16:04:56.374742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18631c636a42081b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:04:56.452709Z",
     "start_time": "2025-05-24T16:04:56.446146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Rating  Sentiment  \\\n",
       "0                                               good     4.0          2   \n",
       "1  Interesting introduction to the subject matter...     3.0          2   \n",
       "2  I have learned the basic concepts of different...     5.0          2   \n",
       "3  the course is really good but there are issues...     3.0          2   \n",
       "4  Not much as I was expected, not very structure...     2.0          0   \n",
       "\n",
       "                                       clean_Comment  \n",
       "0                                               good  \n",
       "1  Interesting introduction to the subject matter...  \n",
       "2  I have learned the basic concepts of different...  \n",
       "3  the course is really good but there are issues...  \n",
       "4  Not much as I was expected, not very structure...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c03332b7-42a7-401c-8baf-431006573a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./trim_and_clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a553734-ad02-4a18-8aad-66bf80390172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./trim_and_clean_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26558c73-14e3-4a52-b41e-98337ef109a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Interesting introduction to the subject matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>I have learned the basic concepts of different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>the course is really good but there are issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Not much as I was expected, not very structure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95948</th>\n",
       "      <td>95948</td>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Overly-remedial information, but that is not w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95949</th>\n",
       "      <td>95949</td>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Content is quite interesting. Timing of the le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95950</th>\n",
       "      <td>95950</td>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>This subject should be taught as a course in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95951</th>\n",
       "      <td>95951</td>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I cannot get my certificate as ID could not be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95952</th>\n",
       "      <td>95952</td>\n",
       "      <td>no me gusta el curso</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>no me gusta el curso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95953 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            Comment  Rating  \\\n",
       "0               0                                               good     4.0   \n",
       "1               1  Interesting introduction to the subject matter...     3.0   \n",
       "2               2  I have learned the basic concepts of different...     5.0   \n",
       "3               3  the course is really good but there are issues...     3.0   \n",
       "4               4  Not much as I was expected, not very structure...     2.0   \n",
       "...           ...                                                ...     ...   \n",
       "95948       95948  Overly-remedial information, but that is not w...     1.0   \n",
       "95949       95949  Content is quite interesting. Timing of the le...     3.0   \n",
       "95950       95950  This subject should be taught as a course in t...     4.0   \n",
       "95951       95951  I cannot get my certificate as ID could not be...     1.0   \n",
       "95952       95952                               no me gusta el curso     1.0   \n",
       "\n",
       "       Sentiment                                      clean_Comment  \n",
       "0              2                                               good  \n",
       "1              2  Interesting introduction to the subject matter...  \n",
       "2              2  I have learned the basic concepts of different...  \n",
       "3              2  the course is really good but there are issues...  \n",
       "4              0  Not much as I was expected, not very structure...  \n",
       "...          ...                                                ...  \n",
       "95948          0  Overly-remedial information, but that is not w...  \n",
       "95949          2  Content is quite interesting. Timing of the le...  \n",
       "95950          2  This subject should be taught as a course in t...  \n",
       "95951          0  I cannot get my certificate as ID could not be...  \n",
       "95952          0                               no me gusta el curso  \n",
       "\n",
       "[95953 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f72f8897353d34e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:05:40.399497Z",
     "start_time": "2025-05-24T16:04:56.540136Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSentiment\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      3\u001B[0m X_train, X_val, y_train, y_val \u001B[38;5;241m=\u001B[39m train_test_split(X, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,stratify\u001B[38;5;241m=\u001B[39my)\n\u001B[0;32m----> 4\u001B[0m X_train_tokenized \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m X_val_tokenized \u001B[38;5;241m=\u001B[39m tokenizer(X_val, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2867\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2865\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   2866\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 2867\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2868\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2869\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2955\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   2950\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2951\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2952\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2953\u001B[0m         )\n\u001B[1;32m   2954\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 2955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2956\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2957\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2958\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2959\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2960\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2961\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2962\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2964\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2965\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2966\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2973\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2974\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2975\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2976\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2977\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   2978\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   2979\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2997\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   2998\u001B[0m     )\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3156\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3146\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[1;32m   3147\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3148\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3149\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3153\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3154\u001B[0m )\n\u001B[0;32m-> 3156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3158\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3160\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3161\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3162\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3163\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3164\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3165\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3167\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3170\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3171\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3172\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3173\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3174\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3176\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils.py:887\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    885\u001B[0m     ids, pair_ids \u001B[38;5;241m=\u001B[39m ids_or_pair_ids\n\u001B[0;32m--> 887\u001B[0m first_ids \u001B[38;5;241m=\u001B[39m \u001B[43mget_input_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    888\u001B[0m second_ids \u001B[38;5;241m=\u001B[39m get_input_ids(pair_ids) \u001B[38;5;28;01mif\u001B[39;00m pair_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    889\u001B[0m input_ids\u001B[38;5;241m.\u001B[39mappend((first_ids, second_ids))\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils.py:854\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_input_ids\u001B[39m(text):\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 854\u001B[0m         tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    855\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(tokens)\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(text) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(text[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/tokenization_utils.py:697\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.tokenize\u001B[0;34m(self, text, **kwargs)\u001B[0m\n\u001B[1;32m    695\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 697\u001B[0m         tokenized_text\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    698\u001B[0m \u001B[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001B[39;00m\n\u001B[1;32m    699\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokenized_text\n",
      "File \u001B[0;32m~/Study/senti with bert/env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:161\u001B[0m, in \u001B[0;36mBertTokenizer._tokenize\u001B[0;34m(self, text, split_special_tokens)\u001B[0m\n\u001B[1;32m    159\u001B[0m split_tokens \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_basic_tokenize:\n\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbasic_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnever_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    163\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# If the token is part of the never_split set\u001B[39;00m\n\u001B[1;32m    165\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbasic_tokenizer\u001B[38;5;241m.\u001B[39mnever_split:\n\u001B[1;32m    166\u001B[0m             split_tokens\u001B[38;5;241m.\u001B[39mappend(token)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "X = list(df[\"clean_Comment\"])\n",
    "y = list(df[\"Sentiment\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d140ec5163c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:05:45.397156Z",
     "start_time": "2025-05-24T16:05:45.394686Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db0bb422e31bc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:05:55.030496Z",
     "start_time": "2025-05-24T16:05:55.027495Z"
    }
   },
   "outputs": [],
   "source": [
    "len(X_train),len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccbb3a5eba88779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:05:59.994434Z",
     "start_time": "2025-05-24T16:05:59.990936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242f03b33482be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:04.496745Z",
     "start_time": "2025-05-24T16:06:04.494226Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be4c383ac39f1a9",
   "metadata": {},
   "source": [
    "### Accuracy scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088143486d698d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:09.728444Z",
     "start_time": "2025-05-24T16:06:09.725623Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a98438bd9d8159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:18:36.602232Z",
     "start_time": "2025-05-24T16:18:36.540755Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d22e12c13b3b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:15.189444220Z",
     "start_time": "2025-05-24T10:09:21.660645Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./trim_and_clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f2129-e4ad-4c85-8e1c-9e7be8f489a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:15.189444220Z",
     "start_time": "2025-05-24T10:09:21.660645Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914cef2bf7cf07f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:15.190596911Z",
     "start_time": "2025-05-24T09:51:56.415600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40101a2-cbfa-4065-973a-66f0aaaba176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:06:15.191241684Z",
     "start_time": "2025-05-24T09:52:01.930528Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
